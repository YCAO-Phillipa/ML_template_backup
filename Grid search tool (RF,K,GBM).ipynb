{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d294e4b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conda install -c conda-forge lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b11ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5314dc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b20ed237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Regression algorithms\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78f68987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('全国.csv', encoding='gbk')\n",
    "\n",
    "# Convert the relevant columns to categorical data type\n",
    "df['Number'] = df['Number'].astype('category')\n",
    "df['Season'] = df['Season'].astype('category')\n",
    "df['Treating_Process'] = df['Treating_Process'].astype('category')\n",
    "df['Phosphorus_Removing_Agents'] = df['Phosphorus_Removing_Agents'].astype('category')\n",
    "df['Carbon_Source'] = df['Carbon_Source'].astype('category')\n",
    "df['City'] = df['City'].astype('category')\n",
    "\n",
    "# Get the column names\n",
    "column_names = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceadcb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "# Variables to use as X\n",
    "Names_X = [\n",
    "    'CODin', 'CODrem',\n",
    "    'BODin/CODin', 'BODrem',\n",
    "    'NH3-Nin', 'NH3-Nrem',\n",
    "    'SSin', 'SSrem',\n",
    "    'TNin', 'TNrem',\n",
    "    'TPin', 'TPrem',\n",
    "    'Season',\n",
    "    'MonthVolume',\n",
    "    'Carbon_Source',\n",
    "    'Mass_Carbon_Source(kg/m3)',\n",
    "    'Phosphorus_Removing_Agents',\n",
    "    'Mass_Phosphorus_Removing_Agents(kg/m3)',\n",
    "    'Treating_Process',\n",
    "    'City',\n",
    "]\n",
    "# Variable to use as y\n",
    "Names_y = 'DS(kg/m3)'\n",
    "\n",
    "# Define X and y\n",
    "X = df[Names_X]\n",
    "y = df[Names_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43b5fc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of GroupShuffleSplit, spliting the data\n",
    "# Such that the training and test sets have different plants\n",
    "gss = GroupShuffleSplit(n_splits=5, test_size=0.15, random_state=7)\n",
    "\n",
    "# Split the data\n",
    "train_indices, test_indices = next(gss.split(X, y, groups=df['Number']))\n",
    "# Create the training and testing sets\n",
    "X_train, y_train = X.iloc[train_indices], y.iloc[train_indices]\n",
    "X_test, y_test = X.iloc[test_indices], y.iloc[test_indices]\n",
    "\n",
    "# The whole dataframe corresponding to the training set\n",
    "df_train = df.iloc[train_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd77a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "   ('rgr', GradientBoostingRegressor())])\n",
    "\n",
    "# Set the options for estimators and hyperparemeters\n",
    "rgr_list = [\n",
    "    {'rgr': [LinearRegression()]},  # Linear Regression\n",
    "    {'rgr': [Lasso()],\n",
    "     'rgr__alpha': [0.1, 1, 10]},  # LASSO\n",
    "    {'rgr': [KNeighborsRegressor()],\n",
    "     'rgr__n_neighbors': [5, 6, 7]},  # K-nearest Neighbors\n",
    "    {'rgr': [SVR()],\n",
    "     'rgr__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "     'rgr__C': [0.1, 1, 10]},  # Support Vector Machines\n",
    "    {'rgr': [RandomForestRegressor(n_estimators=400)],\n",
    "     'rgr__max_depth': [1, 2, 5],\n",
    "     'rgr__max_features': ['sqrt','log2',None],\n",
    "     'rgr__min_samples_split': [2,3,4,5,6,7,8,9],\n",
    "     'rgr__min_impurity_decrease': [0,0.05,0.1],\n",
    "     'rgr__bootstrap': [True,False]},  # Random Forests\n",
    "    {'rgr': [MLPRegressor(random_state=1)],\n",
    "     'rgr__activation': ['logistic', 'tanh', 'relu'],\n",
    "     'rgr__hidden_layer_sizes': [(100,), (15,)],\n",
    "     'rgr__alpha': [1e-4, 1e-5]},  # Neural Networks\n",
    "    {'rgr': [GradientBoostingRegressor(),\n",
    "              LGBMRegressor(), XGBRegressor(),\n",
    "              CatBoostRegressor(silent=True)],\n",
    "     'rgr__n_estimators': [100, 200, 500],\n",
    "     'rgr__max_depth': [3,4,5,6,9],\n",
    "     'rgr__learning_rate': [0.01, 0.05, 0.1, 0.2]},  # Gradient Boosting Machines\n",
    "    {'rgr': [LGBMRegressor()],\n",
    "     'rgr__n_estimators': [100, 200, 500],\n",
    "     'rgr__max_depth': [3,4,5,6,9],\n",
    "     'rgr__reg_lambda': [0,0.1,1,2,3],\n",
    "     'rgr__learning_rate': [0.01, 0.05, 0.1, 0.2]}  # LightGBM\n",
    "]\n",
    "\n",
    "# Combine options for features and options for estimators\n",
    "param_grid = [{**rgr} for rgr in rgr_list]\n",
    "\n",
    "# Search\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid,\n",
    "                           cv=gss.split(X_train, y_train, groups=df_train['Number']),\n",
    "                           scoring='r2',n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "047a4632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "600 fits failed out of a total of 5015.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "300 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 420, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 620, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py\", line 988, in fit\n",
      "    train_dmatrix, evals = _wrap_evaluation_matrices(\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py\", line 448, in _wrap_evaluation_matrices\n",
      "    train_dmatrix = create_dmatrix(\n",
      "                    ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py\", line 908, in _create_dmatrix\n",
      "    return DMatrix(**kwargs, nthread=self.n_jobs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 620, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 743, in __init__\n",
      "    handle, feature_names, feature_types = dispatch_data_backend(\n",
      "                                           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\xgboost\\data.py\", line 957, in dispatch_data_backend\n",
      "    return _from_pandas_df(data, enable_categorical, missing, threads,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\xgboost\\data.py\", line 404, in _from_pandas_df\n",
      "    data, feature_names, feature_types = _transform_pandas_df(\n",
      "                                         ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\xgboost\\data.py\", line 378, in _transform_pandas_df\n",
      "    _invalid_dataframe_dtype(data)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\xgboost\\data.py\", line 270, in _invalid_dataframe_dtype\n",
      "    raise ValueError(msg)\n",
      "ValueError: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, The experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:Season: category, Carbon_Source: category, Phosphorus_Removing_Agents: category, Treating_Process: category, City: category\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "300 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 420, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\catboost\\core.py\", line 5807, in fit\n",
      "    return self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\catboost\\core.py\", line 2381, in _fit\n",
      "    train_params = self._prepare_train_params(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\catboost\\core.py\", line 2261, in _prepare_train_params\n",
      "    train_pool = _build_train_pool(X, y, cat_features, text_features, embedding_features, pairs,\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\catboost\\core.py\", line 1499, in _build_train_pool\n",
      "    train_pool = Pool(X, y, cat_features=cat_features, text_features=text_features, embedding_features=embedding_features, pairs=pairs, weight=sample_weight, group_id=group_id,\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\catboost\\core.py\", line 844, in __init__\n",
      "    self._init(data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, weight,\n",
      "  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\catboost\\core.py\", line 1477, in _init\n",
      "    self._init_pool(data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, weight,\n",
      "  File \"_catboost.pyx\", line 4159, in _catboost._PoolBase._init_pool\n",
      "  File \"_catboost.pyx\", line 4209, in _catboost._PoolBase._init_pool\n",
      "  File \"_catboost.pyx\", line 4025, in _catboost._PoolBase._init_features_order_layout_pool\n",
      "  File \"_catboost.pyx\", line 2919, in _catboost._set_features_order_data_pd_data_frame\n",
      "_catboost.CatBoostError: features data: pandas.DataFrame column 'Season' has dtype 'category' but is not in  cat_features list\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [ 0.12632466 -0.0335125  -0.03229376 ...  0.01363971  0.00736102\n",
      " -0.00401924]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set R²: 0.3683617393752232\n",
      "Best hyperparameters: {'rgr': GradientBoostingRegressor(learning_rate=0.01, n_estimators=500), 'rgr__learning_rate': 0.01, 'rgr__max_depth': 3, 'rgr__n_estimators': 500}\n",
      "Best score: 0.16696642500501574\n",
      "Sample Size: 5831\n",
      "\n",
      "Test set R²: 0.29744051496437607\n",
      "Test set Mean Absolute Error: 0.03930696320361723\n",
      "Test set Normalized Mean Absolute Error: 34.86434496098318 percent\n",
      "Sample Size: 826\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Calculate R2 score of the training set\n",
    "R2_train_BestModel = grid_search.score(X_train, y_train)\n",
    "\n",
    "# Print the Training set R2\n",
    "print('\\nTraining set R²:', R2_train_BestModel)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding score\n",
    "print('Best hyperparameters:', grid_search.best_params_)\n",
    "print('Best score:', grid_search.best_score_)\n",
    "print('Sample Size:', len(y_train))\n",
    "\n",
    "# Prediction on the test set\n",
    "y_pred_BestModel = grid_search.predict(X_test)\n",
    "# R2 on the test set\n",
    "R2_Test_BestModel = r2_score(y_test, y_pred_BestModel)\n",
    "# Mean Absolute Error (MAE) on the test set\n",
    "MAE_Test_BestModel = mean_absolute_error(y_test, y_pred_BestModel)\n",
    "# Normalized Mean Absolute Error (NMAE)  on the test set\n",
    "NMAE_Test_BestModel = MAE_Test_BestModel / y_test.mean() * 100\n",
    "\n",
    "# Print results\n",
    "print('\\nTest set R²:', R2_Test_BestModel)\n",
    "print('Test set Mean Absolute Error:', MAE_Test_BestModel)\n",
    "print('Test set Normalized Mean Absolute Error:',\n",
    "      NMAE_Test_BestModel, 'percent')\n",
    "print('Sample Size:', len(y_pred_BestModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d90d987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set R² in KeyCity: 0.13369312671853462\n",
      "Test set Mean Absolute Error in KeyCity: 0.041777016723734295\n",
      "Test set Normalized Mean Absolute Error in KeyCity: 33.59154430236288 percent\n",
      "Sample Size: 70\n"
     ]
    }
   ],
   "source": [
    "# Test in Key Basins\n",
    "KeyCity = [240]\n",
    "mask = X_test['City'].isin(KeyCity)\n",
    "X_test_KeyCity = X_test[mask]\n",
    "y_test_KeyCity = y_test[mask]\n",
    "\n",
    "# Prediction on the KeyCity\n",
    "y_pred_BestModel_KeyCity = grid_search.predict(X_test_KeyCity)\n",
    "# R2 on the test set\n",
    "R2_Test_BestModel_KeyCity = r2_score(y_test_KeyCity, y_pred_BestModel_KeyCity)\n",
    "# Mean Absolute Error (MAE) on the test set\n",
    "MAE_Test_BestModel_KeyCity = mean_absolute_error(y_test_KeyCity, y_pred_BestModel_KeyCity)\n",
    "# Normalized Mean Absolute Error (NMAE) on the test set\n",
    "NMAE_Test_BestModel_KeyCity = MAE_Test_BestModel_KeyCity / y_test_KeyCity.mean() * 100\n",
    "# Print results\n",
    "print('\\nTest set R² in KeyCity:',\n",
    "      R2_Test_BestModel_KeyCity)\n",
    "print('Test set Mean Absolute Error in KeyCity:',\n",
    "      MAE_Test_BestModel_KeyCity)\n",
    "print('Test set Normalized Mean Absolute Error in KeyCity:',\n",
    "      NMAE_Test_BestModel_KeyCity, 'percent')\n",
    "print('Sample Size:', len(y_pred_BestModel_KeyCity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad4cdbe0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GroupShuffleSplit in module sklearn.model_selection._split:\n",
      "\n",
      "class GroupShuffleSplit(ShuffleSplit)\n",
      " |  GroupShuffleSplit(n_splits=5, *, test_size=None, train_size=None, random_state=None)\n",
      " |  \n",
      " |  Shuffle-Group(s)-Out cross-validation iterator\n",
      " |  \n",
      " |  Provides randomized train/test indices to split data according to a\n",
      " |  third-party provided group. This group information can be used to encode\n",
      " |  arbitrary domain specific stratifications of the samples as integers.\n",
      " |  \n",
      " |  For instance the groups could be the year of collection of the samples\n",
      " |  and thus allow for cross-validation against time-based splits.\n",
      " |  \n",
      " |  The difference between LeavePGroupsOut and GroupShuffleSplit is that\n",
      " |  the former generates splits using all subsets of size ``p`` unique groups,\n",
      " |  whereas GroupShuffleSplit generates a user-determined number of random\n",
      " |  test splits, each with a user-determined fraction of unique groups.\n",
      " |  \n",
      " |  For example, a less computationally intensive alternative to\n",
      " |  ``LeavePGroupsOut(p=10)`` would be\n",
      " |  ``GroupShuffleSplit(test_size=10, n_splits=100)``.\n",
      " |  \n",
      " |  Note: The parameters ``test_size`` and ``train_size`` refer to groups, and\n",
      " |  not to samples, as in ShuffleSplit.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <group_shuffle_split>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_splits : int, default=5\n",
      " |      Number of re-shuffling & splitting iterations.\n",
      " |  \n",
      " |  test_size : float, int, default=0.2\n",
      " |      If float, should be between 0.0 and 1.0 and represent the proportion\n",
      " |      of groups to include in the test split (rounded up). If int,\n",
      " |      represents the absolute number of test groups. If None, the value is\n",
      " |      set to the complement of the train size.\n",
      " |      The default will change in version 0.21. It will remain 0.2 only\n",
      " |      if ``train_size`` is unspecified, otherwise it will complement\n",
      " |      the specified ``train_size``.\n",
      " |  \n",
      " |  train_size : float or int, default=None\n",
      " |      If float, should be between 0.0 and 1.0 and represent the\n",
      " |      proportion of the groups to include in the train split. If\n",
      " |      int, represents the absolute number of train groups. If None,\n",
      " |      the value is automatically set to the complement of the test size.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls the randomness of the training and testing indices produced.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.model_selection import GroupShuffleSplit\n",
      " |  >>> X = np.ones(shape=(8, 2))\n",
      " |  >>> y = np.ones(shape=(8, 1))\n",
      " |  >>> groups = np.array([1, 1, 2, 2, 2, 3, 3, 3])\n",
      " |  >>> print(groups.shape)\n",
      " |  (8,)\n",
      " |  >>> gss = GroupShuffleSplit(n_splits=2, train_size=.7, random_state=42)\n",
      " |  >>> gss.get_n_splits()\n",
      " |  2\n",
      " |  >>> print(gss)\n",
      " |  GroupShuffleSplit(n_splits=2, random_state=42, test_size=None, train_size=0.7)\n",
      " |  >>> for i, (train_index, test_index) in enumerate(gss.split(X, y, groups)):\n",
      " |  ...     print(f\"Fold {i}:\")\n",
      " |  ...     print(f\"  Train: index={train_index}, group={groups[train_index]}\")\n",
      " |  ...     print(f\"  Test:  index={test_index}, group={groups[test_index]}\")\n",
      " |  Fold 0:\n",
      " |    Train: index=[2 3 4 5 6 7], group=[2 2 2 3 3 3]\n",
      " |    Test:  index=[0 1], group=[1 1]\n",
      " |  Fold 1:\n",
      " |    Train: index=[0 1 5 6 7], group=[1 1 3 3 3]\n",
      " |    Test:  index=[2 3 4], group=[2 2 2]\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  ShuffleSplit : Shuffles samples to create independent test/train sets.\n",
      " |  \n",
      " |  LeavePGroupsOut : Train set leaves out all possible subsets of `p` groups.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GroupShuffleSplit\n",
      " |      ShuffleSplit\n",
      " |      BaseShuffleSplit\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_splits=5, *, test_size=None, train_size=None, random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  split(self, X, y=None, groups=None)\n",
      " |      Generate indices to split data into training and test set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Training data, where `n_samples` is the number of samples\n",
      " |          and `n_features` is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,), default=None\n",
      " |          The target variable for supervised learning problems.\n",
      " |      \n",
      " |      groups : array-like of shape (n_samples,)\n",
      " |          Group labels for the samples used while splitting the dataset into\n",
      " |          train/test set.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      train : ndarray\n",
      " |          The training set indices for that split.\n",
      " |      \n",
      " |      test : ndarray\n",
      " |          The testing set indices for that split.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Randomized CV splitters may return different results for each call of\n",
      " |      split. You can make the results identical by setting `random_state`\n",
      " |      to an integer.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseShuffleSplit:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  get_n_splits(self, X=None, y=None, groups=None)\n",
      " |      Returns the number of splitting iterations in the cross-validator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      y : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      groups : object\n",
      " |          Always ignored, exists for compatibility.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      n_splits : int\n",
      " |          Returns the number of splitting iterations in the cross-validator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseShuffleSplit:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "help (GroupShuffleSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac4f5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
